{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-win_amd64.whl (7.5 MB)\n",
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp38-cp38-win_amd64.whl (190.5 MB)\n",
      "Collecting torchvision==0.9.0\n",
      "  Downloading torchvision-0.9.0-cp38-cp38-win_amd64.whl (852 kB)\n",
      "Collecting gluoncv\n",
      "  Using cached gluoncv-0.10.5.post0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting decord\n",
      "  Using cached decord-0.6.0-py3-none-win_amd64.whl (24.7 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages (from torch==1.8.0) (4.10.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Collecting pillow>=4.1.1\n",
      "  Downloading pillow-10.2.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-win_amd64.whl (477 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-win_amd64.whl (56 kB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.1.2-py3-none-any.whl (34 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.49.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting yacs\n",
      "  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl (38.6 MB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-win_amd64.whl (157 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "Collecting autocfg\n",
      "  Using cached autocfg-0.0.8-py3-none-any.whl (13 kB)\n",
      "Collecting portalocker\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages (from portalocker->gluoncv) (306)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl (99 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages (from tqdm->gluoncv) (0.4.6)\n",
      "Installing collected packages: numpy, urllib3, tzdata, pyyaml, pytz, pyparsing, pillow, kiwisolver, importlib-resources, idna, fonttools, cycler, contourpy, charset-normalizer, certifi, yacs, tqdm, torch, scipy, requests, portalocker, pandas, opencv-python, matplotlib, autocfg, torchvision, gluoncv, decord\n",
      "Successfully installed autocfg-0.0.8 certifi-2024.2.2 charset-normalizer-3.3.2 contourpy-1.1.1 cycler-0.12.1 decord-0.6.0 fonttools-4.49.0 gluoncv-0.10.5.post0 idna-3.6 importlib-resources-6.1.2 kiwisolver-1.4.5 matplotlib-3.7.5 numpy-1.24.4 opencv-python-4.9.0.80 pandas-2.0.3 pillow-10.2.0 portalocker-2.8.2 pyparsing-3.1.1 pytz-2024.1 pyyaml-6.0.1 requests-2.31.0 scipy-1.10.1 torch-1.8.0 torchvision-0.9.0 tqdm-4.66.2 tzdata-2024.1 urllib3-2.2.1 yacs-0.1.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'c:\\Users\\Owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'c:\\Users\\Owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\Owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\Owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe and convert-onnx-to-caffe2.exe are installed in 'c:\\Users\\Owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib torch==1.8.0 torchvision==0.9.0 gluoncv decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages (9.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Owner\\.pyenv\\pyenv-win\\versions\\3.8.10\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Started with Pre-trained I3D Models on Kinetcis400\n",
    "\n",
    "`Kinetics400 <https://deepmind.com/research/open-source/kinetics>`_  is an action recognition dataset\n",
    "of realistic action videos, collected from YouTube. With 306,245 short trimmed videos\n",
    "from 400 action categories, it is one of the largest and most widely used dataset in the research\n",
    "community for benchmarking state-of-the-art video action recognition models.\n",
    "\n",
    "`I3D <https://arxiv.org/abs/1705.07750>`_ (Inflated 3D Networks) is a widely adopted 3D video\n",
    "classification network. It uses 3D convolution to learn spatiotemporal information directly from videos.\n",
    "I3D is proposed to improve `C3D <https://arxiv.org/abs/1412.0767>`_ (Convolutional 3D Networks) by inflating from 2D models.\n",
    "We can not only reuse the 2D models' architecture (e.g., ResNet, Inception), but also bootstrap\n",
    "the model weights from 2D pretrained models. In this manner, training 3D networks for video\n",
    "classification is feasible and getting much better results.\n",
    "\n",
    "In this tutorial, we will demonstrate how to load a pre-trained I3D model from `gluoncv-model-zoo`\n",
    "and classify a video clip from the Internet or your local disk into one of the 400 action classes.\n",
    "\n",
    "## Step by Step\n",
    "\n",
    "We will try out a pre-trained I3D model on a single video clip.\n",
    "\n",
    "First, please follow the `installation guide <../../index.html#installation>`__\n",
    "to install ``PyTorch`` and ``GluonCV`` if you haven't done so yet.\n",
    "\n",
    "## Simon's Fixes to Installation Instructions\n",
    "\n",
    "1. Use python 3.8\n",
    "2. Run `pip install torch==1.6.0 torchvision==0.7.0 gluoncv decord`\n",
    "3. Run `pip uninstall Pillow`\n",
    "4. Run `pip install Pillow==9.5.0`\n",
    "5. (Optional) install Jupyter lab to run example notebook linked in tutorial `pip install jupyterlab`\n",
    "6. Download the model config to download the pretrained model used in the tutorial (you will need to edit the config file path to where this file is stored on your system when running the code block which loads the model): https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/action-recognition/configuration/resnet50_v1b_kinetics400.yaml\n",
    "7. Run the notebook and check if class 0 (abseiling) is the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import decord\n",
    "import torch\n",
    "\n",
    "from gluoncv.torch.utils.model_utils import download\n",
    "from gluoncv.torch.data.transforms.videotransforms import video_transforms, volume_transforms\n",
    "from gluoncv.torch.engine.config import get_cfg_defaults\n",
    "from gluoncv.torch.model_zoo import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we download a video and extract a 32-frame clip from it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/bryanyzhu/tiny-ucf101/raw/master/abseiling_k400.mp4'\n",
    "video_fname = download(url)\n",
    "vr = decord.VideoReader(video_fname)\n",
    "frame_id_list = range(0, 64, 2)\n",
    "video_data = vr.get_batch(frame_id_list).asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define transformations for the video clip.\n",
    "This transformation function does four things:\n",
    "(1) resize the shorter side of video clip to short_side_size,\n",
    "(2) center crop the video clip to crop_size x crop_size,\n",
    "(3) transpose the video clip to ``num_channels*num_frames*height*width``,\n",
    "and (4) normalize it with mean and standard deviation calculated across all ImageNet images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video data is downloaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "crop_size = 224\n",
    "short_side_size = 256\n",
    "transform_fn = video_transforms.Compose([video_transforms.Resize(short_side_size, interpolation='bilinear'),\n",
    "                                         video_transforms.CenterCrop(size=(crop_size, crop_size)),\n",
    "                                         volume_transforms.ClipToTensor(),\n",
    "                                         video_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "clip_input = transform_fn(video_data)\n",
    "print('Video data is downloaded and preprocessed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load a pre-trained I3D model. Make sure to change the ``pretrained`` in the configuration file to True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\Owner\\.torch\\models\\i3d_resnet50_v1_kinetics400-18545497.pth from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/torch/models/i3d_resnet50_v1_kinetics400-18545497.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109861/109861 [00:04<00:00, 22455.82KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i3d_resnet50_v1_kinetics400 model is successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "config_file = './i3d_resnet50_v1_kinetics400.yaml'\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(config_file)\n",
    "model = get_model(cfg)\n",
    "model.eval()\n",
    "print('%s model is successfully loaded.' % cfg.CONFIG.MODEL.NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we prepare the video clip and feed it to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input video clip is classified as class 0 with confidence interval 0.9991996884346008\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(torch.unsqueeze(clip_input, dim=0)).numpy()\n",
    "\n",
    "# Convert raw logits to probabilities using softmax\n",
    "probs = torch.nn.functional.softmax(torch.tensor(pred), dim=1).numpy()\n",
    "\n",
    "# Get the top predicted class and calculate confidence interval\n",
    "top_class = np.argmax(probs)\n",
    "confidence_interval = np.max(probs) - np.min(probs)\n",
    "\n",
    "print(f'The input video clip is classified as class {top_class} with confidence interval {confidence_interval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our pre-trained model predicts this video clip\n",
    "to be ``abseiling`` action with high confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "If you would like to dive deeper into finetuing SOTA video models on your datasets,\n",
    "feel free to read the next `tutorial on finetuning <finetune_custom.html>`__.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
